'use client';

// src/lib/useMicrophoneStream.ts
import { checkForAudioTracks, getAudioStream } from "hume";
import { useCallback, useRef, useState } from "react";
var useMicrophoneStream = () => {
  const [permission, setPermission] = useState("prompt");
  const currentStream = useRef(null);
  const getStream = useCallback(
    async (audioConstraints = {}) => {
      let stream;
      try {
        stream = await getAudioStream(audioConstraints);
      } catch (e) {
        if (e instanceof DOMException && "name" in e && e.name === "NotAllowedError") {
          setPermission("denied");
        }
        throw e;
      }
      setPermission("granted");
      checkForAudioTracks(stream);
      currentStream.current = stream;
      return stream;
    },
    []
  );
  const stopStream = useCallback(() => {
    if (currentStream.current) {
      currentStream.current.getTracks().forEach((track) => track.stop());
      currentStream.current = null;
    }
  }, []);
  return {
    getStream,
    stopStream,
    permission
  };
};

// src/lib/useMicrophone.ts
import { getBrowserSupportedMimeType } from "hume";
import { useCallback as useCallback2, useEffect, useRef as useRef2, useState as useState2 } from "react";

// src/lib/convertFrequencyScale.ts
var barkCenterFrequencies = [
  50,
  150,
  250,
  350,
  450,
  570,
  700,
  840,
  1e3,
  1170,
  1370,
  1600,
  1850,
  2150,
  2500,
  2900,
  3400,
  4e3,
  4800,
  5800,
  7e3,
  8500,
  10500,
  13500
];
var minValue = 0;
var maxValue = 255;
function convertLinearFrequenciesToBark(linearData, sampleRate) {
  const maxFrequency = sampleRate / 2;
  const frequencyResolution = maxFrequency / linearData.length;
  const barkFrequencies = barkCenterFrequencies.map((barkFreq) => {
    const linearDataIndex = Math.round(barkFreq / frequencyResolution);
    if (linearDataIndex >= 0 && linearDataIndex < linearData.length) {
      return ((linearData[linearDataIndex] ?? 0) - minValue) / (maxValue - minValue) * 2;
    } else {
      return 0;
    }
  });
  return barkFrequencies;
}

// src/lib/generateEmptyFft.ts
function generateEmptyFft() {
  return Array.from({ length: 24 }).map(() => 0);
}

// src/lib/useMicrophone.ts
var useMicrophone = (props) => {
  const { onAudioCaptured, onError } = props;
  const [isMuted, setIsMuted] = useState2(false);
  const isMutedRef = useRef2(isMuted);
  const currentStream = useRef2(null);
  const [fft, setFft] = useState2(generateEmptyFft());
  const currentAnalyzer = useRef2(null);
  const fftAnimationId = useRef2(null);
  const analyzerSource = useRef2(null);
  const mimeTypeRef = useRef2(null);
  const audioContext = useRef2(null);
  const recorder = useRef2(null);
  const sendAudio = useRef2(onAudioCaptured);
  sendAudio.current = onAudioCaptured;
  const dataHandler = useCallback2((event) => {
    const blob = event.data;
    blob.arrayBuffer().then((buffer) => {
      if (buffer.byteLength > 0) {
        sendAudio.current?.(buffer);
      }
    }).catch((err) => {
      console.log(err);
    });
  }, []);
  const startFftAnalyzer = useCallback2((stream) => {
    if (!audioContext.current) {
      return;
    }
    const source = audioContext.current.createMediaStreamSource(stream);
    analyzerSource.current = source;
    currentAnalyzer.current = audioContext.current.createAnalyser();
    currentAnalyzer.current.fftSize = 2048;
    const bufferLength = currentAnalyzer.current.frequencyBinCount;
    const dataArray = new Uint8Array(bufferLength);
    source.connect(currentAnalyzer.current);
    const draw = () => {
      if (!currentAnalyzer.current || !audioContext.current) {
        return;
      }
      currentAnalyzer.current.getByteFrequencyData(dataArray);
      const sampleRate = audioContext.current.sampleRate;
      const barkFrequencies = convertLinearFrequenciesToBark(
        dataArray,
        sampleRate
      );
      setFft(barkFrequencies);
      fftAnimationId.current = requestAnimationFrame(draw);
    };
    draw();
  }, []);
  const start = useCallback2(
    (stream) => {
      if (!stream) {
        throw new Error("No stream connected");
      }
      if (fftAnimationId.current) {
        cancelAnimationFrame(fftAnimationId.current);
      }
      currentStream.current = stream;
      const context = new AudioContext();
      audioContext.current = context;
      try {
        startFftAnalyzer(stream);
      } catch (e) {
        const message = e instanceof Error ? e.message : "Unknown error";
        console.error(`Failed to start mic analyzer: ${message}`);
      }
      const mimeType = mimeTypeRef.current;
      if (!mimeType) {
        throw new Error("No MimeType specified");
      }
      recorder.current = new MediaRecorder(stream, {
        mimeType
      });
      recorder.current.addEventListener("dataavailable", dataHandler);
      recorder.current.start(100);
    },
    [dataHandler, startFftAnalyzer]
  );
  const stop = useCallback2(async () => {
    if (analyzerSource.current) {
      analyzerSource.current.disconnect();
      analyzerSource.current = null;
    }
    if (currentAnalyzer.current) {
      if (fftAnimationId.current) {
        cancelAnimationFrame(fftAnimationId.current);
      }
      fftAnimationId.current = null;
      currentAnalyzer.current = null;
    }
    if (audioContext.current) {
      await audioContext.current.close().then(() => {
        audioContext.current = null;
      }).catch(() => {
        return null;
      });
    }
    recorder.current?.stop();
    recorder.current?.removeEventListener("dataavailable", dataHandler);
    recorder.current = null;
    currentStream.current?.getTracks().forEach((track) => track.stop());
    setIsMuted(false);
  }, [dataHandler]);
  const stopMicWithRetries = async (maxAttempts = 3, delayMs = 500) => {
    for (let attempt = 1; attempt <= maxAttempts; attempt++) {
      try {
        await stop();
        return;
      } catch (e) {
        if (attempt < maxAttempts) {
          await new Promise((res) => setTimeout(res, delayMs));
        } else {
          const message = e instanceof Error ? e.message : "Unknown error";
          onError?.(
            `Failed to stop mic after ${maxAttempts} attempts: ${message}`,
            "mic_closure_failure"
          );
        }
      }
    }
  };
  const mute = useCallback2(() => {
    if (currentAnalyzer.current) {
      setFft(generateEmptyFft());
    }
    currentStream.current?.getTracks().forEach((track) => {
      track.enabled = false;
    });
    isMutedRef.current = true;
    setIsMuted(true);
  }, []);
  const unmute = useCallback2(() => {
    currentStream.current?.getTracks().forEach((track) => {
      track.enabled = true;
    });
    isMutedRef.current = false;
    setIsMuted(false);
  }, [currentStream]);
  useEffect(() => {
    return () => {
      try {
        recorder.current?.stop();
        recorder.current?.removeEventListener("dataavailable", dataHandler);
        if (currentAnalyzer.current) {
          analyzerSource.current?.disconnect();
          if (fftAnimationId.current) {
            cancelAnimationFrame(fftAnimationId.current);
          }
          fftAnimationId.current = null;
          currentAnalyzer.current = null;
        }
        currentStream.current?.getTracks().forEach((track) => track.stop());
        currentStream.current = null;
      } catch (e) {
        console.log(e);
      }
    };
  }, [dataHandler, currentStream]);
  useEffect(() => {
    const mimeTypeResult = getBrowserSupportedMimeType();
    if (mimeTypeResult.success) {
      mimeTypeRef.current = mimeTypeResult.mimeType;
    } else {
      onError(mimeTypeResult.error.message, "mime_types_not_supported");
    }
  }, [onError]);
  return {
    start,
    stop: stopMicWithRetries,
    mute,
    unmute,
    isMuted,
    fft
  };
};

// src/lib/useSoundPlayer.ts
import { convertBase64ToBlob } from "hume";
import { useCallback as useCallback3, useRef as useRef3, useState as useState3 } from "react";
import z from "zod";

// src/utils/loadAudioWorklet.ts
var loadAudioWorklet = async (ctx, attemptNumber = 1) => {
  return ctx.audioWorklet.addModule(
    `https://storage.googleapis.com/evi-react-sdk-assets/audio-worklet-20250702.js`
  ).then(() => {
    return true;
  }).catch(() => {
    if (attemptNumber >= 10) {
      return false;
    }
    return loadAudioWorklet(ctx, attemptNumber + 1);
  });
};

// src/lib/useSoundPlayer.ts
var useSoundPlayer = (props) => {
  const [isPlaying, setIsPlaying] = useState3(false);
  const [isAudioMuted, setIsAudioMuted] = useState3(false);
  const [volume, setVolumeState] = useState3(1);
  const [fft, setFft] = useState3(generateEmptyFft());
  const audioContext = useRef3(null);
  const analyserNode = useRef3(null);
  const gainNode = useRef3(null);
  const workletNode = useRef3(null);
  const isInitialized = useRef3(false);
  const isProcessing = useRef3(false);
  const frequencyDataIntervalId = useRef3(null);
  const onPlayAudio = useRef3(props.onPlayAudio);
  onPlayAudio.current = props.onPlayAudio;
  const onStopAudio = useRef3(props.onStopAudio);
  onStopAudio.current = props.onStopAudio;
  const onError = useRef3(props.onError);
  onError.current = props.onError;
  const isWorkletActive = useRef3(false);
  const chunkBufferQueues = useRef3({});
  const lastQueuedChunk = useRef3(null);
  const clipQueue = useRef3([]);
  const [queueLength, setQueueLength] = useState3(0);
  const currentlyPlayingAudioBuffer = useRef3(
    null
  );
  const playNextClip = useCallback3(() => {
    if (clipQueue.current.length === 0 || isProcessing.current) {
      setQueueLength(0);
      return;
    }
    if (analyserNode.current === null || audioContext.current === null) {
      onError.current(
        "Audio player is not initialized",
        "audio_player_initialization_failure"
      );
      return;
    }
    const nextClip = clipQueue.current.shift();
    setQueueLength(clipQueue.current.length);
    if (!nextClip) return;
    isProcessing.current = true;
    setIsPlaying(true);
    const bufferSource = audioContext.current.createBufferSource();
    bufferSource.buffer = nextClip.buffer;
    bufferSource.connect(analyserNode.current);
    currentlyPlayingAudioBuffer.current = bufferSource;
    const updateFrequencyData = () => {
      try {
        const bufferSampleRate = bufferSource.buffer?.sampleRate;
        if (!analyserNode.current || typeof bufferSampleRate === "undefined")
          return;
        const dataArray = new Uint8Array(
          analyserNode.current.frequencyBinCount
        );
        analyserNode.current.getByteFrequencyData(dataArray);
        const barkFrequencies = convertLinearFrequenciesToBark(
          dataArray,
          bufferSampleRate
        );
        setFft(() => barkFrequencies);
      } catch (e) {
        setFft(generateEmptyFft());
      }
    };
    frequencyDataIntervalId.current = window.setInterval(
      updateFrequencyData,
      5
    );
    bufferSource.start(0);
    if (nextClip.index === 0) {
      onPlayAudio.current(nextClip.id);
    }
    bufferSource.onended = () => {
      if (frequencyDataIntervalId.current) {
        clearInterval(frequencyDataIntervalId.current);
        frequencyDataIntervalId.current = null;
      }
      setFft(generateEmptyFft());
      bufferSource.disconnect();
      isProcessing.current = false;
      setIsPlaying(false);
      onStopAudio.current(nextClip.id);
      currentlyPlayingAudioBuffer.current = null;
      playNextClip();
    };
  }, []);
  const initPlayer = useCallback3(async () => {
    isWorkletActive.current = true;
    try {
      const initAudioContext = new AudioContext();
      audioContext.current = initAudioContext;
      const analyser = initAudioContext.createAnalyser();
      const gain = initAudioContext.createGain();
      analyser.fftSize = 2048;
      analyser.connect(gain);
      gain.connect(initAudioContext.destination);
      analyserNode.current = analyser;
      gainNode.current = gain;
      if (props.enableAudioWorklet) {
        const isWorkletLoaded = await loadAudioWorklet(initAudioContext);
        if (!isWorkletLoaded) {
          onError.current(
            "Failed to load audio worklet",
            "audio_worklet_load_failure"
          );
          return;
        }
        const worklet = new AudioWorkletNode(
          initAudioContext,
          "audio-processor"
        );
        worklet.connect(analyser);
        workletNode.current = worklet;
        worklet.port.onmessage = (e) => {
          const playingEvent = z.object({
            type: z.literal("start_clip"),
            id: z.string(),
            index: z.number()
          }).safeParse(e.data);
          if (playingEvent.success) {
            if (playingEvent.data.index === 0) {
              onPlayAudio.current(playingEvent.data.id);
            }
            setIsPlaying(true);
          }
          const endedEvent = z.object({ type: z.literal("ended") }).safeParse(e.data);
          if (endedEvent.success) {
            setIsPlaying(false);
            onStopAudio.current("stream");
          }
          const queueLengthEvent = z.object({ type: z.literal("queueLength"), length: z.number() }).safeParse(e.data);
          if (queueLengthEvent.success) {
            if (queueLengthEvent.data.length === 0) {
              setIsPlaying(false);
            }
            setQueueLength(queueLengthEvent.data.length);
          }
          const closedEvent = z.object({ type: z.literal("worklet_closed") }).safeParse(e.data);
          if (closedEvent.success) {
            isWorkletActive.current = false;
          }
        };
        frequencyDataIntervalId.current = window.setInterval(() => {
          const dataArray = new Uint8Array(analyser.frequencyBinCount);
          analyser.getByteFrequencyData(dataArray);
          const barkFrequencies = convertLinearFrequenciesToBark(
            dataArray,
            initAudioContext.sampleRate
          );
          setFft(() => barkFrequencies);
        }, 5);
        isInitialized.current = true;
      } else {
        isInitialized.current = true;
      }
    } catch (e) {
      onError.current(
        "Failed to initialize audio player",
        "audio_player_initialization_failure"
      );
    }
  }, [props.enableAudioWorklet]);
  const convertToAudioBuffer = useCallback3(
    async (message) => {
      if (!isInitialized.current || !audioContext.current) {
        onError.current(
          "Audio player has not been initialized",
          "audio_player_not_initialized"
        );
        return;
      }
      const blob = convertBase64ToBlob(message.data);
      const arrayBuffer = await blob.arrayBuffer();
      const audioBuffer = await audioContext.current.decodeAudioData(arrayBuffer);
      return audioBuffer;
    },
    []
  );
  const getNextAudioBuffers = useCallback3(
    (message, audioBuffer) => {
      if (!chunkBufferQueues.current[message.id]) {
        chunkBufferQueues.current[message.id] = [];
      }
      const queueForCurrMessage = chunkBufferQueues.current[message.id] || [];
      queueForCurrMessage[message.index] = audioBuffer;
      const lastId = lastQueuedChunk.current?.id;
      const buffers = [];
      if (message.id !== lastId) {
        if (queueForCurrMessage[0]) {
          lastQueuedChunk.current = { id: message.id, index: 0 };
          buffers.push({
            id: message.id,
            index: 0,
            buffer: queueForCurrMessage[0]
          });
          queueForCurrMessage[0] = void 0;
        } else {
          return [];
        }
      }
      let nextIdx = (lastQueuedChunk.current?.index || 0) + 1;
      let nextBuf = queueForCurrMessage[nextIdx];
      while (nextBuf) {
        buffers.push({ index: nextIdx, buffer: nextBuf, id: message.id });
        queueForCurrMessage[nextIdx] = void 0;
        lastQueuedChunk.current = { id: message.id, index: nextIdx };
        nextIdx += 1;
        nextBuf = queueForCurrMessage[nextIdx];
      }
      return buffers;
    },
    []
  );
  const addToQueue = useCallback3(
    async (message) => {
      if (!isInitialized.current || !audioContext.current) {
        onError.current(
          "Audio player has not been initialized",
          "audio_player_not_initialized"
        );
        return;
      }
      const audioBuffer = await convertToAudioBuffer(message);
      if (!audioBuffer) {
        onError.current(
          "Failed to convert data to audio buffer",
          "malformed_audio"
        );
        return;
      }
      const playableBuffers = getNextAudioBuffers(message, audioBuffer);
      if (playableBuffers.length === 0) {
        return;
      }
      try {
        for (const nextAudioBufferToPlay of playableBuffers) {
          if (props.enableAudioWorklet) {
            const pcmData = nextAudioBufferToPlay.buffer.getChannelData(0);
            workletNode.current?.port.postMessage({
              type: "audio",
              data: pcmData,
              id: nextAudioBufferToPlay.id,
              index: nextAudioBufferToPlay.index
            });
          } else if (!props.enableAudioWorklet) {
            clipQueue.current.push({
              id: nextAudioBufferToPlay.id,
              buffer: nextAudioBufferToPlay.buffer,
              index: nextAudioBufferToPlay.index
            });
            setQueueLength(clipQueue.current.length);
            if (clipQueue.current.length === 1) {
              playNextClip();
            }
          }
        }
      } catch (e) {
        const eMessage = e instanceof Error ? e.message : "Unknown error";
        onError.current(
          `Failed to add clip to queue: ${eMessage}`,
          "malformed_audio"
        );
      }
    },
    [
      convertToAudioBuffer,
      getNextAudioBuffers,
      playNextClip,
      props.enableAudioWorklet
    ]
  );
  const stopAll = useCallback3(async () => {
    isInitialized.current = false;
    isProcessing.current = false;
    setIsPlaying(false);
    setIsAudioMuted(false);
    setVolumeState(1);
    setFft(generateEmptyFft());
    chunkBufferQueues.current = {};
    lastQueuedChunk.current = null;
    if (frequencyDataIntervalId.current) {
      window.clearInterval(frequencyDataIntervalId.current);
    }
    if (props.enableAudioWorklet) {
      workletNode.current?.port.postMessage({ type: "fadeAndClear" });
      workletNode.current?.port.postMessage({ type: "end" });
      let closed = 0;
      while (closed < 5) {
        if (isWorkletActive.current === false) {
          break;
        }
        closed += 1;
        await new Promise((resolve) => setTimeout(resolve, 100));
      }
      isWorkletActive.current = false;
      if (workletNode.current) {
        workletNode.current.port.close();
        workletNode.current.disconnect();
        workletNode.current = null;
      }
    } else if (!props.enableAudioWorklet) {
      if (currentlyPlayingAudioBuffer.current) {
        currentlyPlayingAudioBuffer.current.disconnect();
        currentlyPlayingAudioBuffer.current = null;
      }
      clipQueue.current = [];
      setQueueLength(0);
    }
    if (analyserNode.current) {
      analyserNode.current.disconnect();
      analyserNode.current = null;
    }
    if (audioContext.current) {
      await audioContext.current.close().then(() => {
        audioContext.current = null;
      }).catch(() => {
        return null;
      });
    }
  }, [props.enableAudioWorklet]);
  const stopAllWithRetries = async (maxAttempts = 3, delayMs = 500) => {
    for (let attempt = 1; attempt <= maxAttempts; attempt++) {
      try {
        await stopAll();
        return;
      } catch (e) {
        if (attempt < maxAttempts) {
          await new Promise((res) => setTimeout(res, delayMs));
        } else {
          const message = e instanceof Error ? e.message : "Unknown error";
          onError.current?.(
            `Failed to stop audio player after ${maxAttempts} attempts: ${message}`,
            "audio_player_closure_failure"
          );
        }
      }
    }
  };
  const clearQueue = useCallback3(() => {
    if (props.enableAudioWorklet) {
      workletNode.current?.port.postMessage({
        type: "fadeAndClear"
      });
    } else if (!props.enableAudioWorklet) {
      if (currentlyPlayingAudioBuffer.current) {
        currentlyPlayingAudioBuffer.current.stop();
        currentlyPlayingAudioBuffer.current = null;
      }
      clipQueue.current = [];
      setQueueLength(0);
    }
    isProcessing.current = false;
    setIsPlaying(false);
    setFft(generateEmptyFft());
  }, [props.enableAudioWorklet]);
  const setVolume = useCallback3(
    (newLevel) => {
      const clampedLevel = Math.max(0, Math.min(newLevel, 1));
      setVolumeState(clampedLevel);
      if (gainNode.current && audioContext.current && !isAudioMuted) {
        gainNode.current.gain.setValueAtTime(
          clampedLevel,
          audioContext.current.currentTime
        );
      }
    },
    [isAudioMuted]
  );
  const muteAudio = useCallback3(() => {
    if (gainNode.current && audioContext.current) {
      gainNode.current.gain.setValueAtTime(0, audioContext.current.currentTime);
      setIsAudioMuted(true);
    }
  }, []);
  const unmuteAudio = useCallback3(() => {
    if (gainNode.current && audioContext.current) {
      gainNode.current.gain.setValueAtTime(
        volume,
        audioContext.current.currentTime
      );
      setIsAudioMuted(false);
    }
  }, [volume]);
  return {
    addToQueue,
    fft,
    initPlayer,
    isPlaying,
    isAudioMuted,
    muteAudio,
    unmuteAudio,
    stopAll: stopAllWithRetries,
    clearQueue,
    volume,
    setVolume,
    queueLength
  };
};

// src/lib/useVoiceClient.ts
import { Hume, HumeClient } from "hume";
import { useCallback as useCallback4, useRef as useRef4, useState as useState4 } from "react";
var isNever = (_n) => {
  return;
};
var VoiceReadyState = /* @__PURE__ */ ((VoiceReadyState2) => {
  VoiceReadyState2["IDLE"] = "idle";
  VoiceReadyState2["CONNECTING"] = "connecting";
  VoiceReadyState2["OPEN"] = "open";
  VoiceReadyState2["CLOSED"] = "closed";
  return VoiceReadyState2;
})(VoiceReadyState || {});
var useVoiceClient = (props) => {
  const connectAbortController = useRef4(null);
  const client = useRef4(null);
  const [readyState, setReadyState] = useState4(
    "idle" /* IDLE */
  );
  const onAudioMessage = useRef4(
    props.onAudioMessage
  );
  onAudioMessage.current = props.onAudioMessage;
  const onMessage = useRef4(props.onMessage);
  onMessage.current = props.onMessage;
  const onToolCall = useRef4(props.onToolCall);
  onToolCall.current = props.onToolCall;
  const onClientError = useRef4(props.onClientError);
  onClientError.current = props.onClientError;
  const onToolCallError = useRef4(
    props.onToolCallError
  );
  onToolCallError.current = props.onToolCallError;
  const onOpen = useRef4(props.onOpen);
  onOpen.current = props.onOpen;
  const onClose = useRef4(props.onClose);
  onClose.current = props.onClose;
  const connect = useCallback4(
    (config, sessionSettings) => {
      connectAbortController.current?.abort();
      const controller = new AbortController();
      const signal = controller.signal;
      connectAbortController.current = controller;
      return new Promise((resolve, reject) => {
        if (signal.aborted) {
          reject(new Error("Connection attempt has already been aborted"));
        }
        const hostname = config.hostname || "api.hume.ai";
        const hume = new HumeClient(
          config.auth.type === "apiKey" ? {
            apiKey: config.auth.value,
            environment: hostname
          } : {
            accessToken: config.auth.value,
            environment: hostname
          }
        );
        const socket = hume.empathicVoice.chat.connect({
          ...config,
          reconnectAttempts: 0
        });
        client.current = socket;
        const abortHandler = () => {
          socket.close();
          reject(new Error("Connection attempt has been aborted"));
        };
        signal.addEventListener("abort", abortHandler);
        socket.on("message", (message) => {
          if (signal.aborted) {
            return;
          }
          if (message.type === "audio_output") {
            const messageWithReceivedAt = {
              ...message,
              receivedAt: /* @__PURE__ */ new Date()
            };
            onAudioMessage.current?.(messageWithReceivedAt);
            return;
          }
          if (message.type === "chat_metadata") {
            onOpen.current?.();
            setReadyState("open" /* OPEN */);
            signal.removeEventListener("abort", abortHandler);
            if (sessionSettings) {
              socket.sendSessionSettings(sessionSettings);
            }
            resolve("open" /* OPEN */);
          }
          if (message.type === "assistant_message" || message.type === "user_message" || message.type === "user_interruption" || message.type === "error" || message.type === "tool_response" || message.type === "tool_error" || message.type === "chat_metadata" || message.type === "assistant_end" || message.type === "assistant_prosody") {
            const messageWithReceivedAt = {
              ...message,
              receivedAt: /* @__PURE__ */ new Date()
            };
            onMessage.current?.(messageWithReceivedAt);
            return;
          }
          if (message.type === "tool_call") {
            const messageWithReceivedAt = {
              ...message,
              receivedAt: /* @__PURE__ */ new Date()
            };
            onMessage.current?.(messageWithReceivedAt);
            if (message.toolType === Hume.empathicVoice.ToolType.Function) {
              void onToolCall.current?.(
                {
                  ...messageWithReceivedAt,
                  // we have to do this because even though we are using the correct
                  // enum on line 30 for the type definition
                  // fern exports an interface and a value using the same `ToolType`
                  // identifier so the type comparisons will always fail
                  toolType: "function"
                },
                {
                  success: (content) => ({
                    type: "tool_response",
                    toolCallId: messageWithReceivedAt.toolCallId,
                    content: JSON.stringify(content)
                  }),
                  error: ({
                    error,
                    code,
                    level,
                    content
                  }) => ({
                    type: "tool_error",
                    toolCallId: messageWithReceivedAt.toolCallId,
                    error,
                    code,
                    level: level !== null ? "warn" : void 0,
                    // level can only be warn
                    content
                  })
                }
              ).then((response) => {
                if (response.type === "tool_response") {
                  socket.sendToolResponseMessage(response);
                } else if (response.type === "tool_error") {
                  socket.sendToolErrorMessage(response);
                } else {
                  onToolCallError.current?.(
                    "Invalid response from tool call"
                  );
                }
              });
            }
            return;
          }
          isNever(message);
          return;
        });
        socket.on("close", (event) => {
          signal.removeEventListener("abort", abortHandler);
          onClose.current?.(event);
          setReadyState("closed" /* CLOSED */);
        });
        socket.on("error", (e) => {
          signal.removeEventListener("abort", abortHandler);
          const message = e instanceof Error ? e.message : "Unknown error";
          onClientError.current?.(message, e instanceof Error ? e : void 0);
          reject(e);
        });
        setReadyState("connecting" /* CONNECTING */);
      });
    },
    []
  );
  const disconnect = useCallback4(() => {
    connectAbortController.current?.abort();
    connectAbortController.current = null;
    setReadyState("idle" /* IDLE */);
    client.current?.close();
  }, []);
  const sendSessionSettings = useCallback4(
    (sessionSettings) => {
      if (readyState !== "open" /* OPEN */) {
        return;
      }
      client.current?.sendSessionSettings(sessionSettings);
    },
    [readyState]
  );
  const sendAudio = useCallback4(
    (arrayBuffer) => {
      if (readyState !== "open" /* OPEN */) {
        return;
      }
      client.current?.socket?.send(arrayBuffer);
    },
    [readyState]
  );
  const sendUserInput = useCallback4(
    (text) => {
      if (readyState !== "open" /* OPEN */) {
        return;
      }
      client.current?.sendUserInput(text);
    },
    [readyState]
  );
  const sendAssistantInput = useCallback4(
    (text) => {
      if (readyState !== "open" /* OPEN */) {
        return;
      }
      client.current?.sendAssistantInput({
        text
      });
    },
    [readyState]
  );
  const sendToolMessage = useCallback4(
    (toolMessage) => {
      if (readyState !== "open" /* OPEN */) {
        return;
      }
      if (toolMessage.type === "tool_error") {
        client.current?.sendToolErrorMessage(toolMessage);
      } else {
        client.current?.sendToolResponseMessage(toolMessage);
      }
    },
    [readyState]
  );
  const sendPauseAssistantMessage = useCallback4(() => {
    if (readyState !== "open" /* OPEN */) {
      return;
    }
    client.current?.pauseAssistant({});
  }, [readyState]);
  const sendResumeAssistantMessage = useCallback4(() => {
    if (readyState !== "open" /* OPEN */) {
      return;
    }
    client.current?.resumeAssistant({});
  }, [readyState]);
  return {
    readyState,
    sendSessionSettings,
    sendAudio,
    connect,
    disconnect,
    sendUserInput,
    sendAssistantInput,
    sendToolMessage,
    sendPauseAssistantMessage,
    sendResumeAssistantMessage
  };
};

// src/lib/VoiceProvider.tsx
import {
  createContext,
  useCallback as useCallback8,
  useContext,
  useEffect as useEffect3,
  useMemo,
  useRef as useRef6,
  useState as useState8
} from "react";

// src/lib/noop.ts
var noop = () => {
};

// src/lib/useCallDuration.ts
import { intervalToDuration } from "date-fns";
import { useCallback as useCallback5, useEffect as useEffect2, useRef as useRef5, useState as useState5 } from "react";
var useCallDuration = () => {
  const interval = useRef5(null);
  const startTime = useRef5(null);
  const [timestamp, setTimestamp] = useState5(null);
  const start = useCallback5(() => {
    startTime.current = Date.now();
    setTimestamp("00:00:00");
    interval.current = window.setInterval(() => {
      if (startTime.current) {
        const duration = intervalToDuration({
          start: startTime.current,
          end: Date.now()
        });
        const hours = (duration.hours ?? 0).toString().padStart(2, "0");
        const minutes = (duration.minutes ?? 0).toString().padStart(2, "0");
        const seconds = (duration.seconds ?? 0).toString().padStart(2, "0");
        setTimestamp(`${hours}:${minutes}:${seconds}`);
      }
    }, 500);
  }, []);
  const stop = useCallback5(() => {
    if (interval.current) {
      window.clearInterval(interval.current);
      interval.current = null;
    }
  }, []);
  const reset = useCallback5(() => {
    setTimestamp(null);
  }, []);
  useEffect2(() => {
    return () => {
      if (interval.current) {
        window.clearInterval(interval.current);
        interval.current = null;
      }
    };
  }, []);
  return { timestamp, start, stop, reset };
};

// src/lib/useMessages.ts
import { useCallback as useCallback6, useState as useState6 } from "react";

// src/utils/index.ts
var keepLastN = (n, arr) => {
  if (arr.length <= n) {
    return arr;
  }
  return arr.slice(arr.length - n);
};

// src/lib/useMessages.ts
var useMessages = ({
  sendMessageToParent,
  messageHistoryLimit
}) => {
  const [voiceMessageMap, setVoiceMessageMap] = useState6({});
  const [messages, setMessages] = useState6([]);
  const [lastVoiceMessage, setLastVoiceMessage] = useState6(null);
  const [lastUserMessage, setLastUserMessage] = useState6(null);
  const [lastAssistantProsodyMessage, setLastAssistantProsodyMessage] = useState6(null);
  const [chatMetadata, setChatMetadata] = useState6(
    null
  );
  const createConnectMessage = useCallback6(() => {
    setChatMetadata(null);
    setMessages(
      (prev) => prev.concat([
        {
          type: "socket_connected",
          receivedAt: /* @__PURE__ */ new Date()
        }
      ])
    );
  }, []);
  const createDisconnectMessage = useCallback6((event) => {
    setMessages(
      (prev) => prev.concat([
        {
          type: "socket_disconnected",
          code: event.code,
          reason: event.reason,
          receivedAt: /* @__PURE__ */ new Date()
        }
      ])
    );
  }, []);
  const findMostRecentUserMessage = useCallback6(
    (allMessages) => {
      let mostRecentUserMessage;
      let mostRecentUserMessageIndex;
      for (let i = allMessages.length - 1; i >= 0; i--) {
        const m = allMessages[i];
        if (m && m.type === "user_message") {
          mostRecentUserMessage = m;
          mostRecentUserMessageIndex = i;
          break;
        }
      }
      return { mostRecentUserMessage, mostRecentUserMessageIndex };
    },
    []
  );
  const updateMessagesArray = useCallback6(
    (messageToAdd) => {
      setMessages((prev) => {
        const { mostRecentUserMessage, mostRecentUserMessageIndex } = findMostRecentUserMessage(prev);
        if (mostRecentUserMessage?.interim === true) {
          const nextMessages = prev.filter((m, idx) => {
            if (idx === mostRecentUserMessageIndex) {
              return false;
            }
            return true;
          });
          return keepLastN(
            messageHistoryLimit,
            nextMessages.concat([messageToAdd, mostRecentUserMessage])
          );
        }
        return keepLastN(messageHistoryLimit, prev.concat([messageToAdd]));
      });
    },
    [findMostRecentUserMessage, messageHistoryLimit]
  );
  const onMessage = useCallback6(
    (message) => {
      switch (message.type) {
        case "assistant_message":
          setVoiceMessageMap((prev) => ({
            ...prev,
            [`${message.id}`]: message
          }));
          break;
        case "user_message":
          sendMessageToParent?.(message);
          if (message.interim === false) {
            setLastUserMessage(message);
          }
          setMessages((prev) => {
            if (prev.length === 0) {
              return keepLastN(messageHistoryLimit, [message]);
            }
            const { mostRecentUserMessage, mostRecentUserMessageIndex } = findMostRecentUserMessage(prev);
            if (mostRecentUserMessage?.interim === true) {
              const nextMessages = prev.filter((m, idx) => {
                if (idx === mostRecentUserMessageIndex) {
                  return false;
                }
                return true;
              });
              return keepLastN(
                messageHistoryLimit,
                nextMessages.concat([message])
              );
            }
            return keepLastN(messageHistoryLimit, prev.concat([message]));
          });
          break;
        case "user_interruption":
        case "error":
        case "tool_call":
        case "tool_response":
        case "tool_error":
        case "assistant_end":
          sendMessageToParent?.(message);
          updateMessagesArray(message);
          break;
        case "assistant_prosody":
          setLastAssistantProsodyMessage(message);
          sendMessageToParent?.(message);
          updateMessagesArray(message);
          break;
        case "chat_metadata":
          sendMessageToParent?.(message);
          updateMessagesArray(message);
          setChatMetadata(message);
          break;
        default:
          break;
      }
    },
    [
      findMostRecentUserMessage,
      messageHistoryLimit,
      sendMessageToParent,
      updateMessagesArray
    ]
  );
  const onPlayAudio = useCallback6(
    (id) => {
      const matchingTranscript = voiceMessageMap[id];
      if (matchingTranscript) {
        sendMessageToParent?.(matchingTranscript);
        setLastVoiceMessage(matchingTranscript);
        updateMessagesArray(matchingTranscript);
        setVoiceMessageMap((prev) => {
          const newMap = { ...prev };
          delete newMap[id];
          return newMap;
        });
      }
    },
    [voiceMessageMap, sendMessageToParent, updateMessagesArray]
  );
  const clearMessages = useCallback6(() => {
    setMessages([]);
    setLastVoiceMessage(null);
    setLastUserMessage(null);
    setLastAssistantProsodyMessage(null);
    setVoiceMessageMap({});
    setChatMetadata(null);
  }, []);
  return {
    createConnectMessage,
    createDisconnectMessage,
    onMessage,
    onPlayAudio,
    clearMessages,
    messages,
    lastVoiceMessage,
    lastUserMessage,
    lastAssistantProsodyMessage,
    chatMetadata
  };
};

// src/lib/useToolStatus.ts
import { useCallback as useCallback7, useState as useState7 } from "react";
var useToolStatus = () => {
  const [store, setStore] = useState7({});
  const addToStore = useCallback7(
    (message) => {
      setStore((prev) => {
        const entry = {
          ...prev[message.toolCallId]
        };
        if (message.type === "tool_call") {
          entry.call = message;
        }
        if (message.type === "tool_response" || message.type === "tool_error") {
          entry.resolved = message;
        }
        return {
          ...prev,
          [message.toolCallId]: entry
        };
      });
    },
    []
  );
  const clearStore = useCallback7(() => {
    setStore({});
  }, []);
  return {
    store,
    addToStore,
    clearStore
  };
};

// src/lib/VoiceProvider.tsx
import { jsx } from "react/jsx-runtime";
var VoiceContext = createContext(null);
var useVoice = () => {
  const ctx = useContext(VoiceContext);
  if (!ctx) {
    throw new Error("useVoice must be used within an VoiceProvider");
  }
  return ctx;
};
var VoiceProvider = ({
  children,
  clearMessagesOnDisconnect = true,
  messageHistoryLimit = 100,
  enableAudioWorklet = true,
  ...props
}) => {
  const {
    timestamp: callDurationTimestamp,
    start: startTimer,
    stop: stopTimer
  } = useCallDuration();
  const [status, setStatus] = useState8({
    value: "disconnected"
  });
  const isConnectingRef = useRef6(false);
  const resourceStatusRef = useRef6({
    mic: "disconnected",
    audioPlayer: "disconnected",
    socket: "disconnected"
  });
  const [isPaused, setIsPaused] = useState8(false);
  const [error, setError] = useState8(null);
  const isError = error !== null;
  const isMicrophoneError = error?.type === "mic_error";
  const isSocketError = error?.type === "socket_error";
  const isAudioError = error?.type === "audio_error";
  const onError = useRef6(props.onError ?? noop);
  onError.current = props.onError ?? noop;
  const onClose = useRef6(props.onClose ?? noop);
  onClose.current = props.onClose ?? noop;
  const onMessage = useRef6(props.onMessage ?? noop);
  onMessage.current = props.onMessage ?? noop;
  const onAudioReceived = useRef6(props.onAudioReceived ?? noop);
  onAudioReceived.current = props.onAudioReceived ?? noop;
  const onAudioStart = useRef6(props.onAudioStart ?? noop);
  onAudioStart.current = props.onAudioStart ?? noop;
  const onAudioEnd = useRef6(props.onAudioEnd ?? noop);
  onAudioEnd.current = props.onAudioEnd ?? noop;
  const onInterruption = useRef6(props.onInterruption ?? noop);
  onInterruption.current = props.onInterruption ?? noop;
  const toolStatus = useToolStatus();
  const messageStore = useMessages({
    sendMessageToParent: onMessage.current,
    messageHistoryLimit
  });
  const checkIsDisconnected = useCallback8(() => {
    return resourceStatusRef.current.mic === "disconnected" || resourceStatusRef.current.audioPlayer === "disconnected" || resourceStatusRef.current.socket === "disconnected";
  }, []);
  const checkIsDisconnecting = useCallback8(() => {
    return resourceStatusRef.current.mic === "disconnecting" || resourceStatusRef.current.audioPlayer === "disconnecting" || resourceStatusRef.current.socket === "disconnecting";
  }, []);
  const updateError = useCallback8((err) => {
    setError(err);
    if (err !== null) {
      onError.current?.(err);
    }
  }, []);
  const onClientError = useCallback8(
    (msg, err) => {
      stopTimer();
      const message = `A websocket connection could not be established. Error message: ${msg ?? "unknown"}`;
      updateError({
        type: "socket_error",
        reason: "socket_connection_failure",
        message,
        error: err
      });
    },
    [stopTimer, updateError]
  );
  const config = props;
  const micStopFnRef = useRef6(null);
  const player = useSoundPlayer({
    enableAudioWorklet,
    onError: (message, reason) => {
      if (checkIsDisconnecting() || checkIsDisconnected()) {
        return;
      }
      updateError({ type: "audio_error", reason, message });
    },
    onPlayAudio: (id) => {
      messageStore.onPlayAudio(id);
      onAudioStart.current(id);
    },
    onStopAudio: (id) => {
      onAudioEnd.current(id);
    }
  });
  const { getStream, stopStream } = useMicrophoneStream();
  const client = useVoiceClient({
    onAudioMessage: (message) => {
      if (checkIsDisconnecting() || checkIsDisconnected()) {
        return;
      }
      void player.addToQueue(message);
      onAudioReceived.current(message);
    },
    onMessage: useCallback8(
      (message) => {
        if (checkIsDisconnecting() || checkIsDisconnected()) {
          return;
        }
        messageStore.onMessage(message);
        if (message.type === "user_interruption" || message.type === "user_message") {
          if (player.isPlaying) {
            onInterruption.current(message);
          }
          player.clearQueue();
        }
        if (message.type === "tool_call" || message.type === "tool_response" || message.type === "tool_error") {
          toolStatus.addToStore(message);
        }
        if (message.type === "error") {
          const error2 = {
            type: "socket_error",
            reason: "received_assistant_error_message",
            message: message.message
          };
          onError.current?.(error2);
        }
      },
      [
        checkIsDisconnected,
        checkIsDisconnecting,
        messageStore,
        player,
        toolStatus
      ]
    ),
    onClientError,
    onToolCallError: useCallback8(
      (message, err) => {
        const error2 = {
          type: "socket_error",
          reason: "received_tool_call_error",
          message,
          error: err
        };
        updateError(error2);
      },
      [updateError]
    ),
    onOpen: useCallback8(() => {
      startTimer();
      messageStore.createConnectMessage();
      props.onOpen?.();
    }, [messageStore, props, startTimer]),
    onClose: useCallback8(
      (event) => {
        stopTimer();
        isConnectingRef.current = false;
        resourceStatusRef.current.socket = "disconnected";
        messageStore.createDisconnectMessage(event);
        if (clearMessagesOnDisconnect) {
          messageStore.clearMessages();
        }
        toolStatus.clearStore();
        setIsPaused(false);
        const resourceShutdownFns = [];
        if (resourceStatusRef.current.audioPlayer === "connected") {
          resourceShutdownFns.push(player.stopAll());
        }
        if (resourceStatusRef.current.mic === "connected") {
          resourceShutdownFns.push(micStopFnRef.current?.());
        }
        if (resourceShutdownFns.length > 0) {
          void Promise.all(resourceShutdownFns).then(() => {
            resourceStatusRef.current.audioPlayer = "disconnected";
            resourceStatusRef.current.mic = "disconnected";
            setStatus({ value: "disconnected" });
            onClose.current?.(event);
          });
        } else {
          onClose.current?.(event);
        }
      },
      [clearMessagesOnDisconnect, messageStore, player, stopTimer, toolStatus]
    ),
    onToolCall: props.onToolCall
  });
  const {
    sendAudio: clientSendAudio,
    sendUserInput: clientSendUserInput,
    sendAssistantInput: clientSendAssistantInput,
    sendSessionSettings: clientSendSessionSettings,
    sendToolMessage: clientSendToolMessage,
    sendPauseAssistantMessage,
    sendResumeAssistantMessage
  } = client;
  const mic = useMicrophone({
    onAudioCaptured: useCallback8(
      (arrayBuffer) => {
        if (resourceStatusRef.current.socket === "disconnecting" || resourceStatusRef.current.socket === "disconnected") {
          return;
        }
        try {
          clientSendAudio(arrayBuffer);
        } catch (e) {
          const message = e instanceof Error ? e.message : "Unknown error";
          updateError({
            type: "socket_error",
            reason: "failed_to_send_audio",
            message
          });
        }
      },
      [clientSendAudio, updateError]
    ),
    onError: useCallback8(
      (message, reason) => {
        updateError({ type: "mic_error", reason, message });
      },
      [updateError]
    )
  });
  useEffect3(() => {
    micStopFnRef.current = mic.stop;
  }, [mic]);
  const { clearQueue } = player;
  const pauseAssistant = useCallback8(() => {
    try {
      sendPauseAssistantMessage();
      setIsPaused(true);
    } catch (e) {
      const message = e instanceof Error ? e.message : "Unknown error";
      updateError({
        type: "socket_error",
        reason: "failed_to_send_message",
        message
      });
    }
    clearQueue();
  }, [sendPauseAssistantMessage, clearQueue, updateError]);
  const resumeAssistant = useCallback8(() => {
    try {
      sendResumeAssistantMessage();
      setIsPaused(false);
    } catch (e) {
      const message = e instanceof Error ? e.message : "Unknown error";
      updateError({
        type: "socket_error",
        reason: "failed_to_send_message",
        message
      });
    }
  }, [sendResumeAssistantMessage, updateError]);
  const checkShouldContinueConnecting = useCallback8(() => {
    return isConnectingRef.current !== false;
  }, []);
  const connect = useCallback8(
    async (options) => {
      const { audioConstraints, sessionSettings, ...socketConfig } = options;
      if (isConnectingRef.current || status.value === "connected") {
        console.warn(
          "Already connected or connecting to a chat. Ignoring duplicate connection attempt."
        );
        return;
      }
      updateError(null);
      setStatus({ value: "connecting" });
      resourceStatusRef.current.socket = "connecting";
      resourceStatusRef.current.audioPlayer = "connecting";
      resourceStatusRef.current.mic = "connecting";
      isConnectingRef.current = true;
      let stream = null;
      try {
        stream = await getStream(options.audioConstraints);
      } catch (e) {
        const isPermissionDeniedError = e instanceof DOMException && e.name === "NotAllowedError";
        const error2 = {
          type: "mic_error",
          reason: isPermissionDeniedError ? "mic_permission_denied" : "mic_initialization_failure",
          message: e instanceof Error ? e.message : "The microphone could not be initialized."
        };
        updateError(error2);
        return;
      }
      if (!checkShouldContinueConnecting()) {
        console.warn("Connection attempt was canceled. Stopping connection.");
        return;
      }
      try {
        await player.initPlayer();
      } catch (e) {
        resourceStatusRef.current.audioPlayer = "disconnected";
        updateError({
          type: "audio_error",
          reason: "audio_player_initialization_failure",
          message: e instanceof Error ? e.message : "We could not connect to the audio player. Please try again."
        });
        return;
      }
      resourceStatusRef.current.audioPlayer = "connected";
      if (!checkShouldContinueConnecting()) {
        console.warn("Connection attempt was canceled. Stopping connection.");
        return;
      }
      try {
        await client.connect(
          {
            ...socketConfig,
            verboseTranscription: socketConfig.verboseTranscription ?? true
          },
          sessionSettings
        );
      } catch (e) {
        return;
      }
      resourceStatusRef.current.socket = "connected";
      if (!checkShouldContinueConnecting()) {
        console.warn("Connection attempt was canceled. Stopping connection.");
        return;
      }
      try {
        mic.start(stream);
      } catch (e) {
        resourceStatusRef.current.mic = "disconnected";
        updateError({
          type: "mic_error",
          reason: "mic_initialization_failure",
          message: e instanceof Error ? e.message : "We could not connect to the microphone. Please try again."
        });
        return;
      }
      resourceStatusRef.current.mic = "connected";
      setStatus({ value: "connected" });
      isConnectingRef.current = false;
    },
    [
      checkShouldContinueConnecting,
      client,
      getStream,
      mic,
      player,
      status.value,
      updateError
    ]
  );
  const disconnectAndCleanUpResources = useCallback8(async () => {
    resourceStatusRef.current.socket = "disconnecting";
    resourceStatusRef.current.audioPlayer = "disconnecting";
    resourceStatusRef.current.mic = "disconnecting";
    isConnectingRef.current = false;
    stopTimer();
    stopStream();
    await mic.stop();
    resourceStatusRef.current.mic = "disconnected";
    if (client.readyState !== "closed" /* CLOSED */) {
      client.disconnect();
    } else {
      resourceStatusRef.current.socket = "disconnected";
    }
    await player.stopAll();
    resourceStatusRef.current.audioPlayer = "disconnected";
    if (clearMessagesOnDisconnect) {
      messageStore.clearMessages();
    }
    toolStatus.clearStore();
    setIsPaused(false);
  }, [
    stopTimer,
    stopStream,
    mic,
    client,
    player,
    clearMessagesOnDisconnect,
    toolStatus,
    messageStore
  ]);
  const disconnect = useCallback8(
    async (disconnectOnError) => {
      await disconnectAndCleanUpResources();
      if (status.value !== "error" && !disconnectOnError) {
        setStatus({ value: "disconnected" });
      }
    },
    [disconnectAndCleanUpResources, status.value]
  );
  useEffect3(() => {
    if (error !== null && status.value !== "error") {
      setStatus({ value: "error", reason: error.message });
      void disconnectAndCleanUpResources();
    }
  }, [status.value, disconnect, disconnectAndCleanUpResources, error]);
  useEffect3(() => {
    return () => {
      void disconnectAndCleanUpResources().then(() => {
        setStatus({ value: "disconnected" });
        isConnectingRef.current = false;
        resourceStatusRef.current = {
          mic: "disconnected",
          audioPlayer: "disconnected",
          socket: "disconnected"
        };
      });
    };
  }, []);
  const sendUserInput = useCallback8(
    (text) => {
      if (resourceStatusRef.current.socket !== "connected") {
        console.warn("Socket is not connected. Cannot send user input.");
        return;
      }
      try {
        clientSendUserInput(text);
      } catch (e) {
        const message = e instanceof Error ? e.message : "Unknown error";
        updateError({
          type: "socket_error",
          reason: "failed_to_send_message",
          message
        });
      }
    },
    [clientSendUserInput, updateError]
  );
  const sendAssistantInput = useCallback8(
    (text) => {
      if (resourceStatusRef.current.socket !== "connected") {
        console.warn("Socket is not connected. Cannot send assistant input.");
        return;
      }
      try {
        clientSendAssistantInput(text);
      } catch (e) {
        const message = e instanceof Error ? e.message : "Unknown error";
        updateError({
          type: "socket_error",
          reason: "failed_to_send_message",
          message
        });
      }
    },
    [clientSendAssistantInput, updateError]
  );
  const sendSessionSettings = useCallback8(
    (sessionSettings) => {
      if (resourceStatusRef.current.socket !== "connected") {
        console.warn("Socket is not connected. Cannot send session settings.");
        return;
      }
      try {
        clientSendSessionSettings(sessionSettings);
      } catch (e) {
        const message = e instanceof Error ? e.message : "Unknown error";
        updateError({
          type: "socket_error",
          reason: "failed_to_send_message",
          message
        });
      }
    },
    [clientSendSessionSettings, updateError]
  );
  const sendToolMessage = useCallback8(
    (message) => {
      if (resourceStatusRef.current.socket !== "connected") {
        console.warn("Socket is not connected. Cannot send tool message.");
        return;
      }
      try {
        clientSendToolMessage(message);
      } catch (e) {
        const message2 = e instanceof Error ? e.message : "Unknown error";
        updateError({
          type: "socket_error",
          reason: "failed_to_send_message",
          message: message2
        });
      }
    },
    [clientSendToolMessage, updateError]
  );
  const ctx = useMemo(
    () => ({
      connect,
      disconnect,
      fft: player.fft,
      micFft: mic.fft,
      isMuted: mic.isMuted,
      isAudioMuted: player.isAudioMuted,
      isPlaying: player.isPlaying,
      messages: messageStore.messages,
      lastVoiceMessage: messageStore.lastVoiceMessage,
      lastUserMessage: messageStore.lastUserMessage,
      lastAssistantProsodyMessage: messageStore.lastAssistantProsodyMessage,
      clearMessages: messageStore.clearMessages,
      mute: mic.mute,
      muteAudio: player.muteAudio,
      readyState: client.readyState,
      sendUserInput,
      sendAssistantInput,
      sendSessionSettings,
      pauseAssistant,
      resumeAssistant,
      sendToolMessage,
      status,
      unmute: mic.unmute,
      unmuteAudio: player.unmuteAudio,
      error,
      isAudioError,
      isError,
      isMicrophoneError,
      isSocketError,
      callDurationTimestamp,
      toolStatusStore: toolStatus.store,
      chatMetadata: messageStore.chatMetadata,
      playerQueueLength: player.queueLength,
      isPaused,
      volume: player.volume,
      setVolume: player.setVolume
    }),
    [
      connect,
      disconnect,
      player.fft,
      player.isAudioMuted,
      player.isPlaying,
      player.muteAudio,
      player.unmuteAudio,
      player.queueLength,
      player.volume,
      player.setVolume,
      mic.fft,
      mic.isMuted,
      mic.mute,
      mic.unmute,
      messageStore.messages,
      messageStore.lastVoiceMessage,
      messageStore.lastUserMessage,
      messageStore.lastAssistantProsodyMessage,
      messageStore.clearMessages,
      messageStore.chatMetadata,
      client.readyState,
      sendUserInput,
      sendAssistantInput,
      sendSessionSettings,
      pauseAssistant,
      resumeAssistant,
      sendToolMessage,
      status,
      error,
      isAudioError,
      isError,
      isMicrophoneError,
      isSocketError,
      callDurationTimestamp,
      toolStatus.store,
      isPaused
    ]
  );
  return /* @__PURE__ */ jsx(VoiceContext.Provider, { value: ctx, children });
};

// src/lib/errors.ts
var SocketUnknownMessageError = class extends Error {
  constructor(message) {
    super(`Unknown message type.${message ? " " + message : ""}`);
    this.name = "SocketUnknownMessageError";
  }
};
var isSocketUnknownMessageError = (err) => {
  return err instanceof SocketUnknownMessageError;
};
var SocketFailedToParseMessageError = class extends Error {
  constructor(message) {
    super(
      `Failed to parse message from socket.${message ? " " + message : ""}`
    );
    this.name = "SocketFailedToParseMessageError";
  }
};
var isSocketFailedToParseMessageError = (err) => {
  return err instanceof SocketFailedToParseMessageError;
};

// src/lib/messages.ts
import { SubscribeEvent } from "hume/serialization/resources/empathicVoice/index.js";

// src/lib/audio-message.ts
import z2 from "zod";
var AudioMessageSchema = z2.object({
  type: z2.literal("audio"),
  data: z2.instanceof(ArrayBuffer)
}).transform((obj) => {
  return Object.assign(obj, {
    receivedAt: /* @__PURE__ */ new Date()
  });
});
var parseAudioMessage = async (blob) => {
  return blob.arrayBuffer().then((buffer) => {
    return {
      type: "audio",
      data: buffer,
      receivedAt: /* @__PURE__ */ new Date()
    };
  }).catch(() => {
    return null;
  });
};

// src/lib/messages.ts
var parseMessageData = async (data) => {
  if (data instanceof Blob) {
    const message = await parseAudioMessage(data);
    if (message) {
      return {
        success: true,
        message
      };
    } else {
      return {
        success: false,
        error: new SocketFailedToParseMessageError(
          `Received blob was unable to be converted to ArrayBuffer.`
        )
      };
    }
  }
  if (typeof data !== "string") {
    return {
      success: false,
      error: new SocketFailedToParseMessageError(
        `Expected a string but received ${typeof data}.`
      )
    };
  }
  const parseResponse = SubscribeEvent.parse(data);
  if (!parseResponse.ok) {
    return {
      success: false,
      error: new SocketUnknownMessageError(
        `Received JSON was not a known message type.`
      )
    };
  }
  return {
    success: true,
    message: parseResponse.value
  };
};
var parseMessageType = async (event) => {
  const data = event.data;
  return parseMessageData(data);
};

// src/models/audio.ts
var Channels = /* @__PURE__ */ ((Channels2) => {
  Channels2[Channels2["MONO"] = 1] = "MONO";
  Channels2[Channels2["STEREO"] = 2] = "STEREO";
  return Channels2;
})(Channels || {});
var AudioEncoding = /* @__PURE__ */ ((AudioEncoding2) => {
  AudioEncoding2["LINEAR16"] = "linear16";
  AudioEncoding2["OPUS"] = "opus";
  return AudioEncoding2;
})(AudioEncoding || {});

// src/models/llm.ts
var LanguageModelOption = /* @__PURE__ */ ((LanguageModelOption2) => {
  LanguageModelOption2["CLAUDE_3_OPUS"] = "CLAUDE_3_OPUS";
  LanguageModelOption2["CLAUDE_3_SONNET"] = "CLAUDE_3_SONNET";
  LanguageModelOption2["CLAUDE_3_HAIKU"] = "CLAUDE_3_HAIKU";
  LanguageModelOption2["CLAUDE_21"] = "CLAUDE_21";
  LanguageModelOption2["CLAUDE_INSTANT_12"] = "CLAUDE_INSTANT_12";
  LanguageModelOption2["GPT_4_TURBO_PREVIEW"] = "GPT_4_TURBO_PREVIEW";
  LanguageModelOption2["GPT_35_TURBO_0125"] = "GPT_35_TURBO_0125";
  LanguageModelOption2["GPT_35_TURBO"] = "GPT_35_TURBO";
  LanguageModelOption2["FIREWORKS_MIXTRAL_8X7B"] = "FIREWORKS_MIXTRAL_8X7B";
  return LanguageModelOption2;
})(LanguageModelOption || {});

// src/models/messages.ts
import z3 from "zod";
var TimeSliceSchema = z3.object({
  begin: z3.number(),
  end: z3.number()
});

// src/models/ttsService.ts
var TTSService = /* @__PURE__ */ ((TTSService2) => {
  TTSService2["DEFAULT"] = "hume_ai";
  TTSService2["ELEVEN_LABS"] = "eleven_labs";
  TTSService2["PLAY_HT"] = "play_ht";
  return TTSService2;
})(TTSService || {});
export {
  AudioEncoding,
  Channels,
  LanguageModelOption,
  SocketFailedToParseMessageError,
  SocketUnknownMessageError,
  TTSService,
  TimeSliceSchema,
  VoiceProvider,
  VoiceReadyState,
  isSocketFailedToParseMessageError,
  isSocketUnknownMessageError,
  parseMessageData,
  parseMessageType,
  useMicrophone,
  useMicrophoneStream,
  useSoundPlayer,
  useVoice,
  useVoiceClient
};
//# sourceMappingURL=index.mjs.map