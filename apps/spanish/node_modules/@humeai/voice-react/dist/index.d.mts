import { CloseEvent } from 'hume/core';
import { Hume } from 'hume';
import { Simplify } from 'type-fest';
import z from 'zod';
import { AudioOutput, ToolCallMessage, JsonMessage, AssistantMessage, UserMessage, AssistantProsody, ToolErrorMessage, ToolResponseMessage, ChatMetadata, UserInterruption, AssistantEnd, AudioInput, WebSocketError } from 'hume/api/resources/empathicVoice';
import { PropsWithChildren, FC } from 'react';

type ConnectionMessage = {
    type: 'socket_connected';
    receivedAt: Date;
} | {
    type: 'socket_disconnected';
    code: CloseEvent['code'];
    reason: CloseEvent['reason'];
    receivedAt: Date;
};

declare const AuthStrategySchema: z.ZodUnion<[z.ZodObject<{
    type: z.ZodLiteral<"apiKey">;
    value: z.ZodString;
}, "strip", z.ZodTypeAny, {
    value: string;
    type: "apiKey";
}, {
    value: string;
    type: "apiKey";
}>, z.ZodObject<{
    type: z.ZodLiteral<"accessToken">;
    value: z.ZodString;
}, "strip", z.ZodTypeAny, {
    value: string;
    type: "accessToken";
}, {
    value: string;
    type: "accessToken";
}>]>;
type AuthStrategy = z.infer<typeof AuthStrategySchema>;

type WithReceivedAt<T> = T & {
    receivedAt: Date;
};
type AssistantEndMessage = WithReceivedAt<AssistantEnd>;
type AssistantTranscriptMessage = WithReceivedAt<AssistantMessage>;
type AssistantProsodyMessage = WithReceivedAt<AssistantProsody>;
type AudioMessage$1 = WithReceivedAt<AudioInput>;
type AudioOutputMessage = WithReceivedAt<AudioOutput>;
type ChatMetadataMessage = WithReceivedAt<ChatMetadata>;
type JSONErrorMessage = WithReceivedAt<WebSocketError>;
type JSONMessage = WithReceivedAt<JsonMessage>;
type ToolCall = WithReceivedAt<ToolCallMessage>;
type ToolError = WithReceivedAt<ToolErrorMessage>;
type ToolResponse = WithReceivedAt<ToolResponseMessage>;
type UserInterruptionMessage = WithReceivedAt<UserInterruption>;
type UserTranscriptMessage = WithReceivedAt<UserMessage>;
declare const TimeSliceSchema: z.ZodObject<{
    begin: z.ZodNumber;
    end: z.ZodNumber;
}, "strip", z.ZodTypeAny, {
    begin: number;
    end: number;
}, {
    begin: number;
    end: number;
}>;
type TimeSlice = z.infer<typeof TimeSliceSchema>;

type SocketConfig = {
    auth: AuthStrategy;
    hostname?: string;
} & Hume.empathicVoice.chat.Chat.ConnectArgs;
declare enum VoiceReadyState {
    IDLE = "idle",
    CONNECTING = "connecting",
    OPEN = "open",
    CLOSED = "closed"
}
type ToolCallHandler = (message: Simplify<ToolCall & {
    toolType: typeof Hume.empathicVoice.ToolType.Function;
}>, send: {
    success: (content: unknown) => Hume.empathicVoice.ToolResponseMessage;
    error: (e: {
        error: string;
        code: string;
        level: string;
        content: string;
    }) => Hume.empathicVoice.ToolErrorMessage;
}) => Promise<Hume.empathicVoice.ToolResponseMessage | Hume.empathicVoice.ToolErrorMessage>;
declare const useVoiceClient: (props: {
    onAudioMessage?: (message: AudioOutputMessage) => void;
    onMessage?: (message: JSONMessage) => void;
    onToolCall?: ToolCallHandler;
    onToolCallError?: (message: string, error?: Error) => void;
    onClientError?: (message: string, error?: Error) => void;
    onOpen?: () => void;
    onClose?: Hume.empathicVoice.chat.ChatSocket.EventHandlers['close'];
}) => {
    readyState: VoiceReadyState;
    sendSessionSettings: (sessionSettings: Hume.empathicVoice.SessionSettings) => void;
    sendAudio: (arrayBuffer: ArrayBufferLike) => void;
    connect: (config: SocketConfig, sessionSettings?: Hume.empathicVoice.SessionSettings) => Promise<VoiceReadyState>;
    disconnect: () => void;
    sendUserInput: (text: string) => void;
    sendAssistantInput: (text: string) => void;
    sendToolMessage: (toolMessage: Hume.empathicVoice.ToolResponseMessage | Hume.empathicVoice.ToolErrorMessage) => void;
    sendPauseAssistantMessage: () => void;
    sendResumeAssistantMessage: () => void;
};

type AudioConstraints = {
    echoCancellation?: boolean;
    noiseSuppression?: boolean;
    autoGainControl?: boolean;
};
type ConnectOptions = Omit<SocketConfig, 'reconnectAttempts'> & {
    audioConstraints?: AudioConstraints;
    sessionSettings?: Hume.empathicVoice.SessionSettings;
};

type PermissionStatus = 'prompt' | 'granted' | 'denied';
declare const useMicrophoneStream: () => {
    getStream: (audioConstraints?: AudioConstraints) => Promise<MediaStream>;
    stopStream: () => void;
    permission: PermissionStatus;
};

declare const useToolStatus: () => {
    store: Record<string, {
        call?: ToolCall | undefined;
        resolved?: ToolError | ToolResponse | undefined;
    }>;
    addToStore: (message: ToolCall | ToolResponse | ToolError) => void;
    clearStore: () => void;
};

type SocketErrorReason = 'socket_connection_failure' | 'failed_to_send_audio' | 'failed_to_send_message' | 'received_assistant_error_message' | 'received_tool_call_error';
type AudioPlayerErrorReason = 'audio_player_initialization_failure' | 'audio_worklet_load_failure' | 'audio_player_not_initialized' | 'malformed_audio' | 'audio_player_closure_failure';
type MicErrorReason = 'mic_permission_denied' | 'mic_initialization_failure' | 'mic_closure_failure' | 'mime_types_not_supported';
type VoiceError = {
    type: 'socket_error';
    reason: SocketErrorReason;
    message: string;
    error?: Error;
} | {
    type: 'audio_error';
    reason: AudioPlayerErrorReason;
    message: string;
    error?: Error;
} | {
    type: 'mic_error';
    reason: MicErrorReason;
    message: string;
    error?: Error;
};
type VoiceStatus = {
    value: 'disconnected' | 'connecting' | 'connected';
    reason?: never;
} | {
    value: 'error';
    reason: string;
};
type VoiceContextType = {
    connect: (options: ConnectOptions) => Promise<void>;
    disconnect: () => Promise<void>;
    fft: number[];
    isMuted: boolean;
    isAudioMuted: boolean;
    isPlaying: boolean;
    messages: (JSONMessage | ConnectionMessage)[];
    lastVoiceMessage: AssistantTranscriptMessage | null;
    lastUserMessage: UserTranscriptMessage | null;
    lastAssistantProsodyMessage: AssistantProsodyMessage | null;
    clearMessages: () => void;
    mute: () => void;
    unmute: () => void;
    muteAudio: () => void;
    unmuteAudio: () => void;
    readyState: VoiceReadyState;
    sendUserInput: (text: string) => void;
    sendAssistantInput: (text: string) => void;
    sendSessionSettings: Hume.empathicVoice.chat.ChatSocket['sendSessionSettings'];
    sendToolMessage: (type: Hume.empathicVoice.ToolResponseMessage | Hume.empathicVoice.ToolErrorMessage) => void;
    pauseAssistant: () => void;
    resumeAssistant: () => void;
    status: VoiceStatus;
    micFft: number[];
    error: VoiceError | null;
    isAudioError: boolean;
    isError: boolean;
    isMicrophoneError: boolean;
    isSocketError: boolean;
    callDurationTimestamp: string | null;
    toolStatusStore: ReturnType<typeof useToolStatus>['store'];
    chatMetadata: ChatMetadataMessage | null;
    playerQueueLength: number;
    isPaused: boolean;
    volume: number;
    setVolume: (level: number) => void;
};
type VoiceProviderProps = PropsWithChildren<{
    onMessage?: (message: JSONMessage) => void;
    onError?: (err: VoiceError) => void;
    onOpen?: () => void;
    onClose?: Hume.empathicVoice.chat.ChatSocket.EventHandlers['close'];
    onToolCall?: ToolCallHandler;
    onAudioReceived?: (audioOutputMessage: AudioOutputMessage) => void;
    onAudioStart?: (clipId: string) => void;
    onAudioEnd?: (clipId: string) => void;
    onInterruption?: (message: UserTranscriptMessage | UserInterruptionMessage) => void;
    /**
     * @default true
     * @description Clear messages when the voice is disconnected.
     */
    clearMessagesOnDisconnect?: boolean;
    /**
     * @default 100
     * @description The maximum number of messages to keep in memory.
     */
    messageHistoryLimit?: number;
    enableAudioWorklet?: boolean;
}>;
declare const useVoice: () => VoiceContextType;
declare const VoiceProvider: FC<VoiceProviderProps>;

type MicrophoneProps = {
    onAudioCaptured: (b: ArrayBuffer) => void;
    onStartRecording?: () => void;
    onStopRecording?: () => void;
    onError: (message: string, reason: MicErrorReason) => void;
};
declare const useMicrophone: (props: MicrophoneProps) => {
    start: (stream: MediaStream) => void;
    stop: (maxAttempts?: number, delayMs?: number) => Promise<void>;
    mute: () => void;
    unmute: () => void;
    isMuted: boolean;
    fft: number[];
};

declare const useSoundPlayer: (props: {
    enableAudioWorklet: boolean;
    onError: (message: string, reason: AudioPlayerErrorReason) => void;
    onPlayAudio: (id: string) => void;
    onStopAudio: (id: string) => void;
}) => {
    addToQueue: (message: AudioOutputMessage) => Promise<void>;
    fft: number[];
    initPlayer: () => Promise<void>;
    isPlaying: boolean;
    isAudioMuted: boolean;
    muteAudio: () => void;
    unmuteAudio: () => void;
    stopAll: (maxAttempts?: number, delayMs?: number) => Promise<void>;
    clearQueue: () => void;
    volume: number;
    setVolume: (newLevel: number) => void;
    queueLength: number;
};

declare class SocketUnknownMessageError extends Error {
    constructor(message?: string);
}
/**
 * @name isSocketUnknownMessageError
 * @description
 * Check if an error is a SocketUnknownMessageError.
 * @param err - The error to check.
 * @returns
 * `true` if the error is a SocketUnknownMessageError.
 * @example
 * ```ts
 * if (isSocketUnknownMessageError(err)) {
 * console.error('Unknown message type');
 * }
 * ```
 */
declare const isSocketUnknownMessageError: (err: unknown) => err is SocketUnknownMessageError;
declare class SocketFailedToParseMessageError extends Error {
    constructor(message?: string);
}
/**
 * @name isSocketFailedToParseMessageError
 * @description
 * Check if an error is a SocketFailedToParseMessageError.
 * @param err - The error to check.
 * @returns
 * `true` if the error is a SocketFailedToParseMessageError.
 * @example
 * ```ts
 * if (isSocketFailedToParseMessageError(err)) {
 * console.error('Failed to parse message from socket');
 * }
 * ```
 */
declare const isSocketFailedToParseMessageError: (err: unknown) => err is SocketFailedToParseMessageError;

declare const AudioMessageSchema: z.ZodEffects<z.ZodObject<{
    type: z.ZodLiteral<"audio">;
    data: z.ZodType<ArrayBuffer, z.ZodTypeDef, ArrayBuffer>;
}, "strip", z.ZodTypeAny, {
    type: "audio";
    data: ArrayBuffer;
}, {
    type: "audio";
    data: ArrayBuffer;
}>, {
    type: "audio";
    data: ArrayBuffer;
} & {
    receivedAt: Date;
}, {
    type: "audio";
    data: ArrayBuffer;
}>;
type AudioMessage = z.infer<typeof AudioMessageSchema>;

/**
 * @name parseMessageData
 * @description
 * Parse the data of a message from the socket.
 * @param data - The data to parse.
 * @returns
 * The parsed message data.
 * @example
 * ```ts
 * const message = await parseMessageData(data);
 * ```
 */
declare const parseMessageData: (data: unknown) => Promise<{
    success: true;
    message: Hume.empathicVoice.SubscribeEvent | AudioMessage;
} | {
    success: false;
    error: Error;
}>;
/**
 * @name parseMessageType
 * @description
 * Parse the type of a message from the socket.
 * @param event - The event to parse.
 * @returns
 * The parsed message type.
 * @example
 * ```ts
 * const message = await parseMessageType(event);
 * ```
 */
declare const parseMessageType: (event: MessageEvent) => Promise<{
    success: true;
    message: Hume.empathicVoice.SubscribeEvent | AudioMessage;
} | {
    success: false;
    error: Error;
}>;

declare enum Channels {
    /** Mono */
    MONO = 1,
    /** Stereo */
    STEREO = 2
}
declare enum AudioEncoding {
    /** 16-bit signed little-endian (PCM) */
    LINEAR16 = "linear16",
    /** Ogg Opus */
    OPUS = "opus"
}

declare enum LanguageModelOption {
    CLAUDE_3_OPUS = "CLAUDE_3_OPUS",
    CLAUDE_3_SONNET = "CLAUDE_3_SONNET",
    CLAUDE_3_HAIKU = "CLAUDE_3_HAIKU",
    CLAUDE_21 = "CLAUDE_21",
    CLAUDE_INSTANT_12 = "CLAUDE_INSTANT_12",
    GPT_4_TURBO_PREVIEW = "GPT_4_TURBO_PREVIEW",
    GPT_35_TURBO_0125 = "GPT_35_TURBO_0125",
    GPT_35_TURBO = "GPT_35_TURBO",
    FIREWORKS_MIXTRAL_8X7B = "FIREWORKS_MIXTRAL_8X7B"
}

declare enum TTSService {
    /** Hume's Text-To-Speech */
    DEFAULT = "hume_ai",
    /** ElevenLab's Text-To-Speech */
    ELEVEN_LABS = "eleven_labs",
    /** Play HT's Text-To-Speech */
    PLAY_HT = "play_ht"
}

export { type AssistantEndMessage, type AssistantProsodyMessage, type AssistantTranscriptMessage, type AudioConstraints, AudioEncoding, type AudioMessage$1 as AudioMessage, type AudioOutputMessage, type AudioPlayerErrorReason, Channels, type ChatMetadataMessage, type ConnectOptions, type ConnectionMessage, type JSONErrorMessage, type JSONMessage, LanguageModelOption, type MicErrorReason, type MicrophoneProps, type SocketConfig, type SocketErrorReason, SocketFailedToParseMessageError, SocketUnknownMessageError, TTSService, type TimeSlice, TimeSliceSchema, type ToolCall, type ToolCallHandler, type ToolError, type ToolResponse, type UserInterruptionMessage, type UserTranscriptMessage, type VoiceContextType, VoiceProvider, type VoiceProviderProps, VoiceReadyState, isSocketFailedToParseMessageError, isSocketUnknownMessageError, parseMessageData, parseMessageType, useMicrophone, useMicrophoneStream, useSoundPlayer, useVoice, useVoiceClient };
